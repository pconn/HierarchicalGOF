\documentclass[12pt,english]{article}

\usepackage{amsmath,amssymb,amsthm,epsfig,lineno,rotfloat,psfrag,natbib,caption,setspace,url,bm,geometry}
\usepackage{ecology,algorithm,algorithmic}
\bibliographystyle{ecology}

%\hypersetup{pdfpagemode=UseNone}

%\usepackage{a4wide,amsmath,epsfig,epsf,psfrag}


\def\be{{\ensuremath\mathbf{e}}}
\def\bx{{\ensuremath\mathbf{x}}}
\def\bX{{\ensuremath\mathbf{X}}}
\def\bthet{{\ensuremath\boldsymbol{\theta}}}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}
%\renewcommand{\refname}{\hspace{2.3in} \normalfont \normalsize LITERATURE CITED}
%this tells it to put 'Literature Cited' instead of 'References'
\bibpunct{(}{)}{,}{a}{}{;}
\oddsidemargin 0.0in
\evensidemargin 0.0in
\textwidth 6.5in
\headheight 0.0in
\topmargin 0.0in
\textheight=9.0in
%\renewcommand{\tablename}{\textbf{Table}}
%\renewcommand{\figurename}{\textbf{Figure}}
\renewcommand{\em}{\it}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}
\renewcommand\theequation{A.\arabic{equation}}
\def\VAR{{\rm Var}\,}
\def\COV{{\rm Cov}\,}
\def\Prob{{\rm P}\,}
\def\bfx{{\bf x}}
\def\bfX{{\bf X}}
\def\bfY{{\bf Y}\,}
\def\bfy{{\bf y}}
\def\bfZ{{\bf Z}\,}
\def\bftheta{\boldsymbol{\theta}}
\def\bfeta{\boldsymbol{\eta}}
\def\bfOmega{\boldsymbol{\Omega}}
\def\bfbeta{\boldsymbol{\beta}}
\def\bfSigma{\boldsymbol{\Sigma}}
\def\bfmu{\boldsymbol{\mu}}
\def\bfnu{\boldsymbol{\nu}}
\def\bfepsilon{\boldsymbol{\epsilon}}
\def\R2{\rm I\!R^2}

\begin{document}

\begin{center} \bf {\large A GUIDE TO BAYESIAN MODEL CHECKING FOR ECOLOGISTS}

\vspace{0.7cm}
Paul B. Conn$^{1*}$, Devin S. Johnson$^1$, Perry J. Williams$^{2,3}$, Sharon R. Melin$^1$, and Mevin
    B. Hooten$^{4,2,3}$
\end{center}
\vspace{0.5cm}

\rm
\small

\it $^1$Marine Mammal Laboratory, Alaska Fisheries Science Center,
NOAA National Marine Fisheries Service,
Seattle, Washington 98115 U.S.A.\\

\it $^2$Department of Fish, Wildlife, and Conservation Biology, Colorado State University, Fort Collins, CO 80523 U.S.A.\\

\it $^3$Department of Statistics, Colorado State University, Fort Collins, CO 80523 U.S.A.\\

\it $^4$U.S. Geological Survey, Colorado Cooperative Fish and Wildlife Research Unit, Colorado State University, Fort Collins, CO 80523 U.S.A.\\

\raggedbottom
\vspace{.5in}

\rm
Appendix A: Algorithms for model checking procedures
\bigskip

\vspace{.3in}

\doublespacing

\rm \begin{flushleft}

Here, we provide pseudocode for several algorithms that can be used to conduct Bayesian model checks.  We use the following conventions when describing algorithms:
\begin{itemize}
  \item We use bold symbols to denote vector- or matrix-valued quantities
  \item We use $\textbf{y}$ (with no superscript) to denote observed data, and $\textbf{y}_i^{rep}$ to denote a simulated data set.
  \item We use brackets do designate a probability distribution.  For instance, $[\boldsymbol{\theta}]$ denotes the distribution of $\boldsymbol{\theta}$, and $[\textbf{y}|\boldsymbol{\theta}]$ denotes the conditional distribution of $\textbf{y}$ given $\boldsymbol{\theta}$
  \item We use a left arrow $\leftarrow$ to mean ``set equal to"
  \item The notation $\forall$ means ``for every"
  \item The notation $\in$ means ``in the set"
\end{itemize}


\begin{algorithm}
\caption{Prior predictive check algorithm for computing a Bayesian p-value, $P$ using $m$ samples from the prior distribution. A selection of discrepancy measures $T(\textbf{y},\boldsymbol{\theta})$ is provided in Table 2 of the main article text.}
\label{alg:prior}
\begin{algorithmic}
\STATE $P \leftarrow 0$
\FOR{$i \in 1:m$}
  \STATE Draw $\boldsymbol{\theta}_i \sim [\boldsymbol{\theta}]$
  \STATE Draw $\textbf{y}_i^{rep} \sim [\textbf{y} | \boldsymbol{\theta}_i]$ \STATE Set $T_i^{rep} \leftarrow T(\textbf{y}_i^{rep},\boldsymbol{\theta}_i)$
  \STATE Set $T_i^{obs} \leftarrow T(\textbf{y},\boldsymbol{\theta}_i)$
  \IF{$T_i^{obs} < T_i^{rep}$}
    \STATE $P \leftarrow P+1$
  \ENDIF
\ENDFOR
\STATE $P \leftarrow P/m$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Posterior predictive check algorithm for computing a Bayesian p-value, $P$ using $m$ samples from the prior distribution.  A selection of discrepancy measures $T(\textbf{y},\boldsymbol{\theta})$ is provided in Table 2 of the main article text.}
\label{alg:posterior}
\begin{algorithmic}
\STATE $P \leftarrow 0$
\FOR{$i \in 1:m$}
  \STATE Draw $\boldsymbol{\theta}_i \sim [\boldsymbol{\theta}|{\bf y}]$
  \STATE Draw $\textbf{y}_i^{rep} \sim [\textbf{y} | \boldsymbol{\theta}_i]$ \STATE Set $T_i^{rep} \leftarrow T(\textbf{y}_i^{rep},\boldsymbol{\theta}_i)$
  \STATE Set $T_i^{obs} \leftarrow T(\textbf{y},\boldsymbol{\theta}_i)$
  \IF{$T_i^{obs} < T_i^{rep}$}
    \STATE $P \leftarrow P+1$
  \ENDIF
\ENDFOR
\STATE $P \leftarrow P/m$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Algorithm for conducting a pivotal discrepancy check to assess the distribution of modeled quantities.  If distributional assumptions are reasonable, the cumulative distribution function associated with modeled quantities should be uniformly distributed \citep{Johnson2004,YuanJohnson2012}. Note that $n$ denotes sample size and $m$ denotes the number of posterior samples utilized.  This method relies on binning the pivotal quantity $w_{ij} = g(y_{ij},\boldsymbol{\theta}_i)$ into $K \times L$ bins, where $K$ and $L$ are fixed by the investigator (bins should be chosen to achieve reasonable sample size in each of the $KL$ bin combinations).  We use $\Theta$ to denote the cumulative distribution function for the distribution of the pivotal quantity.  Specific examples of $g()$ and $\Theta$ are provided in the text.  As written, this algorithm assesses the fit of the data distribution $[{\bf y}|\boldsymbol{\theta}$]; however, note that it can be applied to other levels of a hierarchical model.}
\label{alg:pivot}
\begin{algorithmic}
\STATE Set $b_l \leftarrow l/L$ for $l=0,1,\hdots,L$
\STATE Set $O_{ikl} \leftarrow 0 \hspace{2mm} \forall \hspace{2mm} i \in 1:m, k \in 1:K, l \in 1:L$
\STATE Set $n_{ik} \leftarrow 0 \hspace{2mm} \forall \hspace{2mm} i \in 1:m, k \in 1:K$
\FOR{$i \in 1:m$}
  \STATE Draw $\boldsymbol{\theta}_i \sim [\boldsymbol{\theta}|\textbf{y}]$
  \FOR{$j \in 1:n$}
    \STATE $\mu_{ij} \leftarrow \textrm{E}(y_j|\boldsymbol{\theta}_i)$
    \STATE $w_{ij} \leftarrow g(y_{ij},\boldsymbol{\theta}_i)$
    %\STATE $v_{ij} \leftarrow \VAR(y_j|\boldsymbol{\theta}_i)$
    %\STATE $w_{ij} \leftarrow (y_j-\mu_{ij})v_{ij}^{-0.5}$
  \ENDFOR
  \STATE Set $q_0 \leftarrow -\infty, q_{K} \leftarrow \infty$, and $q_h \leftarrow \textrm{quantile}_{h/K*100\%}(\mu_{ij})$ for $h \in 1:(K-1)$ and the quantile is taken over $j \in 1:n$)
  \FOR{$k \in 1:K$}
    \FOR{$j \in 1:n$}
      \IF{$ q_{k-1} \le \mu_{ij} < q_k $}
        \STATE $r_{ij} \leftarrow k$
        \STATE $n_{ik} \leftarrow n_{ik}+1$
      \ENDIF
    \ENDFOR
    \FOR{$l \in 1:L$}
      \IF{$\Theta(w_{ij}) \in (b_{l-1},b_l]$ \& $r_{ij}==k$}
        \STATE $O_{ikl} \leftarrow O_{ikl}+1$
      \ENDIF
    \ENDFOR
    \STATE Set $T_{ik}({\bf y},\bftheta_i) \leftarrow \sum_{l=1}^L \frac{(O_{ikl}-n_{ik}L^{-1})^2}{n_{ik}L^{-1}} $
  \ENDFOR
  \STATE Set $T_i({\bf y},\bftheta_i) \leftarrow \sum_{k=1}^K T_{ik}({\bf y},\bftheta_i)$
\ENDFOR
\STATE Test $T_{ik}({\bf y},\bftheta_i) \sim \chi^2_{L-1}$ for targeted lack-of-fit
\STATE Test $T_{i}({\bf y},\bftheta_i) \sim \chi^2_{K(L-1)}$ for omnibus lack-of-fit
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Algorithm for conducting predictive probability integral transform (PIT) checks, as described by e.g., \citet{Fruiiwirth1996}.  This approach requires having ``test" data; here we assume that a ``leave-one-out" procedure is used, although other  approaches are certainly possible (and may be preferable, especially when sample sizes are large).  To this end, we define $\textbf{y}_{-i}$ as the set of data for which the $i$th observation is missing, $m$ to be the total number of observations, and $n$ to be the number of posterior samples that are analyzed for each data set.  The indicator function $I(A)$ takes on the value 1.0 if the statement $A$ is true, and is 0.0 otherwise.}
\label{alg:PIT}
\begin{algorithmic}
\STATE Set $u_j \leftarrow 0 \: \forall  \: j \in \{ 1,2,\hdots,n \}$
\FOR{$i \in 1:m$}
  \FOR{$j \in 1:n$}
    \STATE Simulate a draw $\boldsymbol{\theta}_{ij}$ from the posterior distribution $[\theta | \textbf{y}_{-i}] \propto [ \textbf{y}_{-i} | \theta][\theta]$
    \STATE Simulate a posterior prediction $y_{ij}^{rep}$ from the predictive density (or mass function), $[y_i | \boldsymbol{\theta}_{ij}]$
  \ENDFOR
    \IF{$y_i$ has continuous support}
      \STATE{Set $u_i \leftarrow n^{-1} \sum_j I(y_{ij}^{rep} \le y_i)$}
    \ENDIF
    \IF{$y_i$ has nonnegative integer support (i.e. for count data)}
      \STATE{Set $u_i \leftarrow n^{-1}\sum_j I(y_{ij}^{rep} < y_i) + 0.5 I(\tilde{y}_{ij}==y_{ij})$}
    \ENDIF
    \IF{$y_i$ has binary support}
      \STATE{Set $u_i \leftarrow n^{-1}\sum_j I(y_{ij}^{rep}==y_{ij})$}
    \ENDIF
\ENDFOR
\STATE Divergence from a Uniform(0,1) distribution is indicative of lack of fit.  Very high or very low values may indicate outliers.
\end{algorithmic}
\end{algorithm}

\clearpage
\renewcommand{\refname}{Literature Cited}
% \bibliographystyle{JEcol}

\bibliography{master_bib}

\end{flushleft}
\end{document}














